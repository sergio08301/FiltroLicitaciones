import csv
import email
from dotenv import load_dotenv
import os
import imaplib
import re
from licitacion import Licitacion
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
import shutil
import time

#Configuraci√≥n
dias= 5                                                #dias que puede ir atr√°s en correo para buscar licitaciones
asunto= "Correu diari de subscriptors generals"         #Asunto que quieres buscar en los correos
colorEmpleador="#660303"                                #Color en el cual esta escrito el empleador en el correo

# Cargar variables de entorno (.env)
load_dotenv()
EMAIL_USER = os.getenv("EMAIL_USER")
EMAIL_PASS = os.getenv("EMAIL_PASS")

# Cambia este si usas otro proveedor de correo
IMAP_SERVER = "imap.gmail.com"

from dataclasses import dataclass


def connect_to_email():
    print("Conectando al correo...")
    mail = imaplib.IMAP4_SSL(IMAP_SERVER)
    mail.login(EMAIL_USER, EMAIL_PASS)
    mail.select("inbox")
    return mail

def buscar_correo_por_asunto(mail, asunto_buscado):
    # Obtener todos los IDs de correo hasta la fecha l√≠mite
    fecha_limite = (datetime.now() - timedelta(days=dias)).strftime('%d-%b-%Y')
    status, data = mail.search(None, 'SINCE', fecha_limite)
    if status != "OK":
        print("‚ùå No se pudieron recuperar los correos.")
        return None

    email_ids = data[0].split()
    email_ids.reverse()  # Buscar desde el m√°s reciente

    for eid in email_ids:
        status, msg_data = mail.fetch(eid, "(RFC822)")
        if status != "OK":
            continue

        raw_email = msg_data[0][1]
        message = email.message_from_bytes(raw_email)
        subject = message["Subject"]
        print("Mensaje a analizar: "+subject)

        if subject and asunto.lower() in subject.lower():
            print("\n‚úÖ Correo encontrado:")
            print("Asunto:", subject)
            fecha_raw = message["Date"]
            fecha_formateada = parsedate_to_datetime(fecha_raw).strftime('%d/%m/%Y')    #formatear la fecha
            print("Fecha:", fecha_formateada)
            return message  # Devuelve el mensaje completo

    print("‚ö†Ô∏è No se encontr√≥ ning√∫n correo con ese asunto.")
    return None

def eliminar_encabezado_reenviado(texto):
    lineas = texto.splitlines()
    resultado = []
    saltando = False
    for linea in lineas:
        if any(linea.strip().lower().startswith(prefix) for prefix in ["de:", "enviado el:", "para:", "asunto:"]):
            saltando = True
            continue
        if saltando and linea.strip() == "":
            saltando = False
            continue
        if not saltando:
            resultado.append(linea)
    return "\n".join(resultado)

def extraer_html_del_mensaje(mensaje):
    if mensaje.is_multipart():
        for part in mensaje.walk():
            if part.get_content_type() == "text/html":
                html = part.get_payload(decode=True).decode(errors="ignore")
                return html
    return None

def extraer_licitaciones_desde_html(html: str) -> list:
    soup = BeautifulSoup(html, "html.parser")
    licitaciones = []
    empleador_actual = None

    for tag in soup.find_all():
        # üü• Detectar empleador por color rojo exacto
        if tag.name in ["b", "strong"] and tag.find("span", style=lambda s: s and colorEmpleador in s.lower()):
            empleador_actual = tag.get_text(strip=True)

        # üîó Detectar enlaces de licitaci√≥n v√°lidos √∫nicamente
        elif tag.name == "a" and "href" in tag.attrs:
            enlace = tag["href"]

            # ‚úÖ Solo procesar enlaces v√°lidos de licitaciones
            if not enlace.startswith("https://contractaciopublica.cat/ca/detall-publicacio/estado/"):
                continue  # Saltar enlaces tipo "feu clic aqu√≠"

            titulo = tag.get_text(strip=True)

            # Obtener los siguientes <p> con info adicional
            siguiente_info = tag.find_parent("p").find_next_siblings("p", limit=3)
            fecha_publicacion, fecha_limite, presupuesto = "", "", ""

            for p in siguiente_info:
                texto = p.get_text(strip=True).lower()
                if "data de publicaci√≥" in texto:
                    fecha_publicacion = texto.split(":", 1)[1].strip()
                elif "termini de presentaci√≥" in texto:
                    fecha_limite = texto.split(":", 1)[1].strip()
                elif "pressupost de licitaci√≥" in texto:
                    presupuesto = texto.split(":", 1)[1].strip()

            lic = Licitacion(
                empleador=empleador_actual or "",
                titulo=titulo,
                enlace=enlace,
                fecha_publicacion=fecha_publicacion,
                fecha_limite=fecha_limite,
                presupuesto=presupuesto,
                administratives="",
                tecniques=""
            )

            licitaciones.append(lic)

    return licitaciones

def parsear_licitaciones(texto):
    # Dividir el texto en bloques de licitaciones (separados por 2+ saltos de l√≠nea)
    bloques = re.split(r'\n', texto.strip())

    licitaciones = []

    for bloque in bloques:
        if not bloque.strip():
            continue

        try:
            # Extraer t√≠tulo (todo hasta el enlace)
            #empleador=

            titulo_match = re.search(r'^(.*?)\s*<https?://', bloque, re.DOTALL)
            titulo = titulo_match.group(1).strip() if titulo_match else "Sin t√≠tulo"

            # Extraer enlace
            enlace_match = re.search(r'<(https?://[^\s>]+)>', bloque)
            enlace = enlace_match.group(1) if enlace_match else "Sin enlace"

            # Extraer fechas y presupuesto
            fecha_pub_match = re.search(r'Data de publicaci√≥:\s*(.*?)\s*h', bloque)
            fecha_pub = fecha_pub_match.group(1) if fecha_pub_match else "Fecha desconocida"

            fecha_lim_match = re.search(r'Termini de presentaci√≥ d\'ofertes:\s*(.*?)\s*h', bloque)
            fecha_lim = fecha_lim_match.group(1) if fecha_lim_match else "Fecha l√≠mite desconocida"

            presupuesto_match = re.search(r'Pressupost de licitaci√≥:\s*([\d.,]+)\s*‚Ç¨', bloque)
            presupuesto = f"{presupuesto_match.group(1)} ‚Ç¨ sense IVA" if presupuesto_match else "Presupuesto no especificado"

            # Crear objeto Licitacion
            licitacion = Licitacion(
                titulo=titulo,
                enlace=enlace,
                fecha_publicacion=fecha_pub,
                fecha_limite=fecha_lim,
                presupuesto=presupuesto
            )

            licitaciones.append(licitacion)

        except Exception as e:
            print(
                f"Error procesando bloque: {e}\n{bloque[:200]}...")  # Mostrar solo el inicio del bloque para no saturar la salida

    return licitaciones


def filtrado_inicial(licitaciones: list) -> list:

    #TODO Meter los primeros filtros, como la fecha o el importe insuficientes

    return licitaciones

def limpiar_nombre(texto):
    texto = texto.lower().strip()
    texto = re.sub(r"[^\w\s-]", "", texto)
    texto = texto.replace(" ", "_")
    return texto[:80]

def descargar_pdfs_por_href(licitacion, carpeta_base="pdfs"):
    url = licitacion.GetEnlace()
    titulo = licitacion.GetTitulo()
    carpeta_licitacion = os.path.join(carpeta_base, limpiar_nombre(titulo))
    os.makedirs(carpeta_licitacion, exist_ok=True)


    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")

    driver = webdriver.Chrome(options=chrome_options)
    driver.get(url)
    time.sleep(3)  # permitir que cargue el DOM

    claves_buscadas = [
        "plec de cl√†usules administratives",
        "plec de prescripcions t√®cniques"
    ]

    try:
        rows = driver.find_elements(By.CLASS_NAME, "row")

        for row in rows:
            try:
                label_div = row.find_element(By.CLASS_NAME, "col-md-4")
                label_text = label_div.text.strip().lower()

                if any(clave in label_text for clave in claves_buscadas):
                    link_div = row.find_element(By.CLASS_NAME, "col-md-8")
                    enlaces = link_div.find_elements(By.TAG_NAME, "a")
                    if not enlaces:
                        print(f"‚ö†Ô∏è No se encontr√≥ enlace en: {label_text}")
                        continue

                    enlace = enlaces[0]
                    href = enlace.get_attribute("href")

                    if href:
                        # Determinar el nombre del archivo seg√∫n la clave detectada
                        if "administratives" in label_text:
                            nombre_archivo = "administratives.pdf"
                        elif "t√®cniques" in label_text:
                            nombre_archivo = "tecniques.pdf"
                        else:
                            continue  # No coincide con las claves conocidas

                        destino = os.path.join(carpeta_licitacion, nombre_archivo)
                        if os.path.exists(destino):
                            print(f" Ya existe: {destino}")
                            continue
                        try:
                            headers = {"User-Agent": "Mozilla/5.0"}
                            r = requests.get(href, headers=headers)
                            with open(destino, "wb") as f:
                                f.write(r.content)
                            if "administratives" in label_text:
                                licitacion.SetAdministratives(destino)
                            elif "t√®cniques" in label_text:
                                licitacion.SetTecniques(destino)
                            print(f"‚úÖ PDF guardado en: {destino}")
                        except Exception as e:
                            print(f"‚ùå Error al descargar desde {href}: {e}")
            except Exception as e:
                continue
    finally:
        driver.quit()


def guardar_licitaciones_csv(licitaciones, ruta_archivo="licitaciones.csv"):
    with open(ruta_archivo, mode="w", newline="", encoding="utf-8") as archivo:
        writer = csv.writer(archivo)
        writer.writerow([
            "Empleador", "Titulo", "Enlace", "FechaPublicacion",
            "FechaLimite", "Presupuesto", "AdministrativesPDF", "TecniquesPDF"
        ])
        for lic in licitaciones:
            writer.writerow([
                lic.GetEmpleador(),
                lic.GetTitulo(),
                lic.GetEnlace(),
                lic.GetFecha_publicacion(),
                lic.GetFecha_limite(),
                lic.GetPresupuesto(),
                lic.GetAdminsitratives,
                lic.GetTecniques,
            ])

def cargar_licitaciones_csv(ruta_archivo="licitaciones.csv"):
    licitaciones = []
    with open(ruta_archivo, mode="r", encoding="utf-8") as archivo:
        reader = csv.DictReader(archivo)
        for fila in reader:
            lic = Licitacion(
                fila["Empleador"],
                fila["Titulo"],
                fila["Enlace"],
                fila["FechaPublicacion"],
                fila["FechaLimite"],
                fila["Presupuesto"],
                fila.get("AdministrativesPDF", "").strip() or None,
                fila.get("TecniquesPDF", "").strip() or None
            )
            licitaciones.append(lic)
    return licitaciones

def main():
    #Encontrar el correo
    csv_path = "licitaciones.csv"

    respuesta = input("¬øQuieres cargar las licitaciones desde archivo CSV? (y/n): ").strip().lower()

    if respuesta == "y" and os.path.exists(csv_path):
        licitaciones = cargar_licitaciones_csv(csv_path)
        print(f"üîÑ {len(licitaciones)} licitaciones cargadas desde {csv_path}")
    else:
        print("‚è© Se continuar√° con el proceso normal (descarga desde correo y scrapping).")

    mail = connect_to_email()
    mensaje = buscar_correo_por_asunto(mail, asunto)
    mail.logout()
    if not mensaje:
        print("‚ùå No se encontr√≥ ning√∫n correo reciente con ese asunto.")
        return

    html = extraer_html_del_mensaje(mensaje)
    if html:
        licitaciones = extraer_licitaciones_desde_html(html)
        print(f"\nüìã Se detectaron {len(licitaciones)} licitaciones del correo:")

    licitaciones_filtradas = filtrado_inicial(licitaciones)
    print(f"\nüìã Se conservan {len(licitaciones_filtradas)} licitaciones despu√©s del primer filtrado")

    for lic in licitaciones_filtradas:
        try:
            descargar_pdfs_por_href(lic)
        except Exception as e:
            print(f"‚ùå Error al procesar licitaci√≥n: {lic.GetTitulo()} ‚Äî {e}")

    guardar_licitaciones_csv(licitaciones_filtradas)

if __name__ == "__main__":
    main()